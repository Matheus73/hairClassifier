{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5365868-3bcb-453c-a00c-5401e06511df",
   "metadata": {},
   "source": [
    "# Hair Detector\n",
    "\n",
    "#### Created by: Matheus Gabriel\n",
    "\n",
    "Project to detect type of hair using one deep learning model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536cac87-a40b-45c8-9fd0-6c7789aa0b12",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "862b5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "from keras.preprocessing import image\n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676b5b4-acf6-4b7d-afe2-ffbafc24720a",
   "metadata": {},
   "source": [
    "## Load images to train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6f874321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 553 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = image.ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True )\n",
    "train_set = train_datagen.flow_from_directory('data/Train/',\n",
    "                                                 target_size = (80, 80),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5e2674cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 218 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = image.ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True )\n",
    "test_set = test_datagen.flow_from_directory('data/Test/',\n",
    "                                                 target_size = (80, 80),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ec5f0-e0b8-488f-9720-e7274bc18dad",
   "metadata": {},
   "source": [
    "## Defining the layers of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ef1ebf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[80, 80, 3]))\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "cnn.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation=tf.keras.layers.LeakyReLU()))\n",
    "\n",
    "cnn.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
    "\n",
    "cnn.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f432a3f-2718-4a92-ad55-8c74b677eedd",
   "metadata": {},
   "source": [
    "## Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "51a267d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18/18 [==============================] - 8s 443ms/step - loss: 1.0758 - accuracy: 0.4485 - val_loss: 1.1185 - val_accuracy: 0.4174\n",
      "Epoch 2/30\n",
      "18/18 [==============================] - 7s 412ms/step - loss: 0.9932 - accuracy: 0.5371 - val_loss: 1.1233 - val_accuracy: 0.3440\n",
      "Epoch 3/30\n",
      "18/18 [==============================] - 7s 406ms/step - loss: 0.9405 - accuracy: 0.5642 - val_loss: 1.0478 - val_accuracy: 0.5000\n",
      "Epoch 4/30\n",
      "18/18 [==============================] - 7s 415ms/step - loss: 0.8892 - accuracy: 0.6022 - val_loss: 1.0107 - val_accuracy: 0.4862\n",
      "Epoch 5/30\n",
      "18/18 [==============================] - 7s 410ms/step - loss: 0.8065 - accuracy: 0.6329 - val_loss: 0.9052 - val_accuracy: 0.5734\n",
      "Epoch 6/30\n",
      "18/18 [==============================] - 7s 412ms/step - loss: 0.8503 - accuracy: 0.6112 - val_loss: 1.0075 - val_accuracy: 0.4725\n",
      "Epoch 7/30\n",
      "18/18 [==============================] - 7s 405ms/step - loss: 0.8101 - accuracy: 0.6600 - val_loss: 0.9549 - val_accuracy: 0.5183\n",
      "Epoch 8/30\n",
      "18/18 [==============================] - 7s 405ms/step - loss: 0.7796 - accuracy: 0.6582 - val_loss: 0.9051 - val_accuracy: 0.5459\n",
      "Epoch 9/30\n",
      "18/18 [==============================] - 7s 399ms/step - loss: 0.7443 - accuracy: 0.6763 - val_loss: 1.1196 - val_accuracy: 0.4954\n",
      "Epoch 10/30\n",
      "18/18 [==============================] - 7s 402ms/step - loss: 0.7599 - accuracy: 0.6745 - val_loss: 0.9151 - val_accuracy: 0.5367\n",
      "Epoch 11/30\n",
      "18/18 [==============================] - 7s 409ms/step - loss: 0.6870 - accuracy: 0.7233 - val_loss: 0.8906 - val_accuracy: 0.5963\n",
      "Epoch 12/30\n",
      "18/18 [==============================] - 7s 406ms/step - loss: 0.6027 - accuracy: 0.7396 - val_loss: 0.8654 - val_accuracy: 0.6055\n",
      "Epoch 13/30\n",
      "18/18 [==============================] - 7s 407ms/step - loss: 0.6039 - accuracy: 0.7432 - val_loss: 0.8099 - val_accuracy: 0.6422\n",
      "Epoch 14/30\n",
      "18/18 [==============================] - 7s 413ms/step - loss: 0.5652 - accuracy: 0.7740 - val_loss: 0.9508 - val_accuracy: 0.6422\n",
      "Epoch 15/30\n",
      "18/18 [==============================] - 7s 406ms/step - loss: 0.5766 - accuracy: 0.7595 - val_loss: 0.9422 - val_accuracy: 0.5872\n",
      "Epoch 16/30\n",
      "18/18 [==============================] - 7s 418ms/step - loss: 0.5275 - accuracy: 0.7830 - val_loss: 0.8426 - val_accuracy: 0.6422\n",
      "Epoch 17/30\n",
      "18/18 [==============================] - 7s 417ms/step - loss: 0.5214 - accuracy: 0.7631 - val_loss: 0.9638 - val_accuracy: 0.6055\n",
      "Epoch 18/30\n",
      "18/18 [==============================] - 7s 407ms/step - loss: 0.4743 - accuracy: 0.8029 - val_loss: 0.8095 - val_accuracy: 0.6606\n",
      "Epoch 19/30\n",
      "18/18 [==============================] - 7s 412ms/step - loss: 0.4360 - accuracy: 0.8264 - val_loss: 0.9316 - val_accuracy: 0.6330\n",
      "Epoch 20/30\n",
      "18/18 [==============================] - 7s 412ms/step - loss: 0.4498 - accuracy: 0.8156 - val_loss: 0.9949 - val_accuracy: 0.5826\n",
      "Epoch 21/30\n",
      "18/18 [==============================] - 7s 403ms/step - loss: 0.4453 - accuracy: 0.8373 - val_loss: 1.0211 - val_accuracy: 0.6193\n",
      "Epoch 22/30\n",
      "18/18 [==============================] - 7s 407ms/step - loss: 0.3920 - accuracy: 0.8481 - val_loss: 0.9350 - val_accuracy: 0.6147\n",
      "Epoch 23/30\n",
      "18/18 [==============================] - 7s 419ms/step - loss: 0.3669 - accuracy: 0.8571 - val_loss: 1.0274 - val_accuracy: 0.6284\n",
      "Epoch 24/30\n",
      "18/18 [==============================] - 7s 411ms/step - loss: 0.3882 - accuracy: 0.8391 - val_loss: 0.9027 - val_accuracy: 0.6330\n",
      "Epoch 25/30\n",
      "18/18 [==============================] - 7s 414ms/step - loss: 0.3424 - accuracy: 0.8879 - val_loss: 1.2215 - val_accuracy: 0.5780\n",
      "Epoch 26/30\n",
      "18/18 [==============================] - 7s 412ms/step - loss: 0.3328 - accuracy: 0.8680 - val_loss: 0.7475 - val_accuracy: 0.7064\n",
      "Epoch 27/30\n",
      "18/18 [==============================] - 7s 405ms/step - loss: 0.3649 - accuracy: 0.8644 - val_loss: 1.0182 - val_accuracy: 0.6422\n",
      "Epoch 28/30\n",
      "18/18 [==============================] - 7s 411ms/step - loss: 0.3562 - accuracy: 0.8680 - val_loss: 1.0345 - val_accuracy: 0.5963\n",
      "Epoch 29/30\n",
      "18/18 [==============================] - 7s 404ms/step - loss: 0.3127 - accuracy: 0.8897 - val_loss: 0.8968 - val_accuracy: 0.6743\n",
      "Epoch 30/30\n",
      "18/18 [==============================] - 7s 409ms/step - loss: 0.2719 - accuracy: 0.9005 - val_loss: 1.0341 - val_accuracy: 0.6972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4655632190>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "cnn.fit(x = train_set, validation_data = test_set, epochs = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2bb160-9aad-49bd-bc01-c79c571b9e5d",
   "metadata": {},
   "source": [
    "## Function to return the type of hair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9764cd95-83be-4acb-96f2-9d9d0b038aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictCabelo(img):\n",
    "    test_image = image.load_img(img, target_size = (80, 80))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    result = cnn.predict(test_image)\n",
    "\n",
    "    if(result[0][0] == max(result[0])):\n",
    "        return 'cacheado'\n",
    "    \n",
    "    if(result[0][1] == max(result[0])):\n",
    "        return 'liso'\n",
    "\n",
    "    if(result[0][2] == max(result[0])):\n",
    "        return'ondulado'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af325b-ab88-4cd1-8ee9-34d59c97c084",
   "metadata": {},
   "source": [
    "## Load `advertising.mp4` to extract frames and generate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "caadd6e6-80d3-4fef-ae39-1f3aba628e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture('advertising.mp4')\n",
    "\n",
    "def getFrame(sec):\n",
    "    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n",
    "    hasFrames,image = vidcap.read()\n",
    "    if hasFrames:\n",
    "        cv2.imwrite(f\"frames/{count}.jpg\", image)\n",
    "        prediction = predictCabelo(f\"frames/{count}.jpg\")\n",
    "        cv2.rectangle(image,(150,15),(500,85),(255,255,255), -1)\n",
    "        cv2.rectangle(image,(150,15),(500,85),(56,142,72), 2)\n",
    "        cv2.putText(image,prediction,(0 + 200,0 + 0 + 50), cv2.FONT_HERSHEY_COMPLEX,1, (56,142,72), 2, cv2.LINE_AA)  \n",
    "        cv2.imwrite(f\"frames/{count}.jpg\", image)\n",
    "    return hasFrames\n",
    "\n",
    "sec = 0\n",
    "frameRate = 0.5\n",
    "count=1\n",
    "success = getFrame(sec)\n",
    "while success:\n",
    "    count = count + 1\n",
    "    sec = sec + frameRate\n",
    "    sec = round(sec, 2)\n",
    "    success = getFrame(sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a494f-1d9e-406d-9ab9-72cffd88c46b",
   "metadata": {},
   "source": [
    "## Join the frames in `video.avi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "86c4ef9d-f976-41a1-8ee4-5ee1626c6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathIn= './frames/'\n",
    "pathOut = 'video.avi'\n",
    "fps = 0.5\n",
    "frame_array = []\n",
    "files = [f for f in os.listdir(pathIn) if isfile(join(pathIn, f))]\n",
    "\n",
    "files.sort(key = lambda x: x[5:-4])\n",
    "files.sort()\n",
    "frame_array = []\n",
    "files = [f for f in os.listdir(pathIn) if isfile(join(pathIn, f))]\n",
    "\n",
    "files.sort(key = lambda x: x[5:-4])\n",
    "for i in range(len(files)):\n",
    "    filename=pathIn + files[i]\n",
    "    img = cv2.imread(filename)\n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    frame_array.append(img)\n",
    "out = cv2.VideoWriter(pathOut,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n",
    "for i in range(len(frame_array)):\n",
    "    # writing to a image array\n",
    "    out.write(frame_array[i])\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
