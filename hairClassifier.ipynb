{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5365868-3bcb-453c-a00c-5401e06511df",
   "metadata": {},
   "source": [
    "# Hair Detector\n",
    "\n",
    "#### Created by: Matheus Gabriel\n",
    "\n",
    "Project to detect type of hair using one deep learning model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536cac87-a40b-45c8-9fd0-6c7789aa0b12",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "862b5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "from keras.preprocessing import image\n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676b5b4-acf6-4b7d-afe2-ffbafc24720a",
   "metadata": {},
   "source": [
    "## Load images to train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f874321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 494 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = image.ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True )\n",
    "train_set = train_datagen.flow_from_directory('data/Train/',\n",
    "                                                 target_size = (80, 80),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e2674cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 218 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = image.ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True )\n",
    "test_set = test_datagen.flow_from_directory('data/Test/',\n",
    "                                                 target_size = (80, 80),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ec5f0-e0b8-488f-9720-e7274bc18dad",
   "metadata": {},
   "source": [
    "## Defining the layers of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef1ebf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[80, 80, 3]))\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "cnn.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation=tf.keras.layers.LeakyReLU()))\n",
    "\n",
    "cnn.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
    "\n",
    "cnn.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f432a3f-2718-4a92-ad55-8c74b677eedd",
   "metadata": {},
   "source": [
    "## Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51a267d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "16/16 [==============================] - 6s 366ms/step - loss: 1.0801 - accuracy: 0.4595 - val_loss: 1.1012 - val_accuracy: 0.3807\n",
      "Epoch 2/30\n",
      "16/16 [==============================] - 5s 337ms/step - loss: 1.0202 - accuracy: 0.5263 - val_loss: 1.0749 - val_accuracy: 0.4128\n",
      "Epoch 3/30\n",
      "16/16 [==============================] - 6s 368ms/step - loss: 0.9740 - accuracy: 0.5486 - val_loss: 1.0274 - val_accuracy: 0.4587\n",
      "Epoch 4/30\n",
      "16/16 [==============================] - 6s 365ms/step - loss: 0.8970 - accuracy: 0.5850 - val_loss: 1.0861 - val_accuracy: 0.4450\n",
      "Epoch 5/30\n",
      "16/16 [==============================] - 6s 362ms/step - loss: 0.8934 - accuracy: 0.5911 - val_loss: 1.0446 - val_accuracy: 0.5138\n",
      "Epoch 6/30\n",
      "16/16 [==============================] - 6s 368ms/step - loss: 0.8273 - accuracy: 0.6457 - val_loss: 1.0352 - val_accuracy: 0.5092\n",
      "Epoch 7/30\n",
      "16/16 [==============================] - 6s 391ms/step - loss: 0.8302 - accuracy: 0.6377 - val_loss: 1.0140 - val_accuracy: 0.5275\n",
      "Epoch 8/30\n",
      "16/16 [==============================] - 6s 402ms/step - loss: 0.8074 - accuracy: 0.6579 - val_loss: 1.0464 - val_accuracy: 0.4862\n",
      "Epoch 9/30\n",
      "16/16 [==============================] - 6s 394ms/step - loss: 0.8123 - accuracy: 0.6437 - val_loss: 0.9610 - val_accuracy: 0.5092\n",
      "Epoch 10/30\n",
      "16/16 [==============================] - 6s 401ms/step - loss: 0.7428 - accuracy: 0.6842 - val_loss: 0.9980 - val_accuracy: 0.5275\n",
      "Epoch 11/30\n",
      "16/16 [==============================] - 6s 397ms/step - loss: 0.7670 - accuracy: 0.6599 - val_loss: 0.9442 - val_accuracy: 0.5321\n",
      "Epoch 12/30\n",
      "16/16 [==============================] - 6s 405ms/step - loss: 0.7030 - accuracy: 0.6822 - val_loss: 0.9809 - val_accuracy: 0.5275\n",
      "Epoch 13/30\n",
      "16/16 [==============================] - 6s 401ms/step - loss: 0.6892 - accuracy: 0.7065 - val_loss: 1.0184 - val_accuracy: 0.5183\n",
      "Epoch 14/30\n",
      "16/16 [==============================] - 6s 398ms/step - loss: 0.6745 - accuracy: 0.7024 - val_loss: 0.9652 - val_accuracy: 0.5321\n",
      "Epoch 15/30\n",
      "16/16 [==============================] - 6s 395ms/step - loss: 0.6255 - accuracy: 0.7247 - val_loss: 0.9384 - val_accuracy: 0.5596\n",
      "Epoch 16/30\n",
      "16/16 [==============================] - 6s 397ms/step - loss: 0.5802 - accuracy: 0.7368 - val_loss: 0.9095 - val_accuracy: 0.6147\n",
      "Epoch 17/30\n",
      "16/16 [==============================] - 6s 400ms/step - loss: 0.5578 - accuracy: 0.7591 - val_loss: 1.1814 - val_accuracy: 0.5183\n",
      "Epoch 18/30\n",
      "16/16 [==============================] - 6s 404ms/step - loss: 0.5919 - accuracy: 0.7672 - val_loss: 0.9482 - val_accuracy: 0.5596\n",
      "Epoch 19/30\n",
      "16/16 [==============================] - 6s 401ms/step - loss: 0.5351 - accuracy: 0.7874 - val_loss: 1.1341 - val_accuracy: 0.5688\n",
      "Epoch 20/30\n",
      "16/16 [==============================] - 6s 398ms/step - loss: 0.5121 - accuracy: 0.7834 - val_loss: 0.9259 - val_accuracy: 0.5734\n",
      "Epoch 21/30\n",
      "16/16 [==============================] - 6s 400ms/step - loss: 0.4730 - accuracy: 0.8077 - val_loss: 1.0208 - val_accuracy: 0.6009\n",
      "Epoch 22/30\n",
      "16/16 [==============================] - 6s 407ms/step - loss: 0.4700 - accuracy: 0.7996 - val_loss: 1.1003 - val_accuracy: 0.5963\n",
      "Epoch 23/30\n",
      "16/16 [==============================] - 6s 403ms/step - loss: 0.4787 - accuracy: 0.8057 - val_loss: 1.0638 - val_accuracy: 0.5963\n",
      "Epoch 24/30\n",
      "16/16 [==============================] - 6s 406ms/step - loss: 0.4480 - accuracy: 0.8502 - val_loss: 1.0571 - val_accuracy: 0.5642\n",
      "Epoch 25/30\n",
      "16/16 [==============================] - 6s 400ms/step - loss: 0.3837 - accuracy: 0.8360 - val_loss: 0.9401 - val_accuracy: 0.6193\n",
      "Epoch 26/30\n",
      "16/16 [==============================] - 6s 400ms/step - loss: 0.4382 - accuracy: 0.8360 - val_loss: 0.9362 - val_accuracy: 0.6284\n",
      "Epoch 27/30\n",
      "16/16 [==============================] - 6s 400ms/step - loss: 0.3718 - accuracy: 0.8462 - val_loss: 1.2500 - val_accuracy: 0.5872\n",
      "Epoch 28/30\n",
      "16/16 [==============================] - 6s 401ms/step - loss: 0.3445 - accuracy: 0.8785 - val_loss: 1.0794 - val_accuracy: 0.5917\n",
      "Epoch 29/30\n",
      "16/16 [==============================] - 7s 415ms/step - loss: 0.3083 - accuracy: 0.8725 - val_loss: 1.0823 - val_accuracy: 0.6193\n",
      "Epoch 30/30\n",
      "16/16 [==============================] - 6s 387ms/step - loss: 0.3305 - accuracy: 0.8806 - val_loss: 1.1833 - val_accuracy: 0.5963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fecf87665e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "cnn.fit(x = train_set, validation_data = test_set, epochs = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2bb160-9aad-49bd-bc01-c79c571b9e5d",
   "metadata": {},
   "source": [
    "## Function to return the type of hair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9764cd95-83be-4acb-96f2-9d9d0b038aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictCabelo(img):\n",
    "    test_image = image.load_img(img, target_size = (80, 80))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    result = cnn.predict(test_image)\n",
    "\n",
    "    if(result[0][0] == max(result[0])):\n",
    "        return 'cacheado'\n",
    "    \n",
    "    if(result[0][1] == max(result[0])):\n",
    "        return 'liso'\n",
    "\n",
    "    if(result[0][2] == max(result[0])):\n",
    "        return'ondulado'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af325b-ab88-4cd1-8ee9-34d59c97c084",
   "metadata": {},
   "source": [
    "## Load `advertising.mp4` to extract frames and generate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caadd6e6-80d3-4fef-ae39-1f3aba628e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture('advertising.mp4')\n",
    "\n",
    "def getFrame(sec):\n",
    "    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000)\n",
    "    hasFrames,image = vidcap.read()\n",
    "    if hasFrames:\n",
    "        cv2.imwrite(f\"frames/{count}.jpg\", image)\n",
    "        prediction = predictCabelo(f\"frames/{count}.jpg\")\n",
    "        cv2.rectangle(image,(150,15),(500,85),(255,255,255), -1)\n",
    "        cv2.rectangle(image,(150,15),(500,85),(56,142,72), 2)\n",
    "        cv2.putText(image,prediction,(0 + 200,0 + 0 + 50), cv2.FONT_HERSHEY_COMPLEX,1, (56,142,72), 2, cv2.LINE_AA)  \n",
    "        cv2.imwrite(f\"frames/{count}.jpg\", image)\n",
    "    return hasFrames\n",
    "\n",
    "sec = 0\n",
    "frameRate = 0.5\n",
    "count=1\n",
    "success = getFrame(sec)\n",
    "while success:\n",
    "    count = count + 1\n",
    "    sec = sec + frameRate\n",
    "    sec = round(sec, 2)\n",
    "    success = getFrame(sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a494f-1d9e-406d-9ab9-72cffd88c46b",
   "metadata": {},
   "source": [
    "## Join the frames in `video.avi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86c4ef9d-f976-41a1-8ee4-5ee1626c6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathIn= './frames/'\n",
    "pathOut = 'video.avi'\n",
    "fps = 0.5\n",
    "frame_array = []\n",
    "files = [f for f in os.listdir(pathIn) if isfile(join(pathIn, f))]\n",
    "\n",
    "files.sort(key = lambda x: x[5:-4])\n",
    "files.sort()\n",
    "frame_array = []\n",
    "files = [f for f in os.listdir(pathIn) if isfile(join(pathIn, f))]\n",
    "\n",
    "files.sort(key = lambda x: x[5:-4])\n",
    "for i in range(len(files)):\n",
    "    filename=pathIn + files[i]\n",
    "    img = cv2.imread(filename)\n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    frame_array.append(img)\n",
    "out = cv2.VideoWriter(pathOut,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n",
    "for i in range(len(frame_array)):\n",
    "    # writing to a image array\n",
    "    out.write(frame_array[i])\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
